- func: paged_attention_v1(Tensor(a!) out, Tensor query, Tensor key_cache, Tensor value_cache, int num_kv_heads, float scale, Tensor block_tables, Tensor context_lens, int block_size, int max_context_len, Tensor? alibi_slopes, str kv_cache_dtype, float k_scale, float v_scale, int tp_rank, int blocksparse_local_blocks, int blocksparse_vert_stride, int blocksparse_block_size, int blocksparse_head_sliding_step, float k_zero, float v_zero, Tensor? out_scales) -> ()
  variants: function

- func: paged_attention_v2(Tensor(a!) out, Tensor exp_sums, Tensor max_logits, Tensor tmp_output, Tensor query, Tensor key_cache, Tensor value_cache, int num_kv_heads, float scale, Tensor block_tables, Tensor context_lens, int block_size, int max_context_len, Tensor? alibi_slopes, str kv_cache_dtype, float k_scale, float v_scale, int tp_rank, int blocksparse_local_blocks, int blocksparse_vert_stride, int blocksparse_block_size, int blocksparse_head_sliding_step, float k_zero, float v_zero, Tensor? out_scales) -> ()
  variants: function

- func: mha_fwd_kvcache_mla(Tensor(a!) q, Tensor kcache, Tensor? vcache, int head_size_v, Tensor seqlens_k, Tensor block_table, float softmax_scale, bool is_causal, Tensor tile_scheduler_metadata, Tensor num_splits) -> (Tensor, Tensor)
  variants: function

- func: silu_and_mul(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: gelu_and_mul(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: gelu_tanh_and_mul(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: gelu_new(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: gelu_fast(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: gelu_quick(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: rms_norm(Tensor(a!) out, Tensor input, Tensor weight, float epsilon) -> ()
  variants: function

- func: fused_add_rms_norm(Tensor(a!) input, Tensor(a!) residual, Tensor weight, float epsilon) -> ()
  variants: function

- func: rotary_embedding(Tensor positions, Tensor(a!) query, Tensor(a!) key, int head_size, Tensor cos_sin_cache, bool is_neox) -> ()
  variants: function

- func: awq_gemm_gcu(Tensor(a!) _in_feats, Tensor(a!) _kernel, Tensor(a!) _scaling_factors, Tensor(a!) _zeros, int split_k_iters, Tensor? bias, int group_size) -> Tensor
  variants: function

- func: awq_dequantize(Tensor(a!) _kernel, Tensor(a!) _scaling_factors, Tensor(a!) _zeros, int split_k_iters, int thx, int thy) -> Tensor
  variants: function

- func: squeezellm_gemm(Tensor(a!) vec, Tensor(a!) mat, Tensor(a!) mul, Tensor(a!) lookup_table) -> ()
  variants: function

- func: gptq_gemm_gcu(Tensor(a!) a, Tensor(a!) b_q_weight, Tensor(a!) b_gptq_qzeros, Tensor(a!) b_gptq_scales, Tensor(a!) b_g_idx, int bit, Tensor? bias, int group_size) -> Tensor
  variants: function

- func: gptq_shuffle(Tensor(a!) q_weight, Tensor(a!) q_perm) -> ()
  variants: function

- func: fused_moe_kernel(Tensor(a!) C, Tensor A, Tensor B, Tensor topk_weights, Tensor topk_ids, Tensor sorted_topk_ids, Tensor expert_ids, Tensor num_tokens_post_pad, bool mul_routed_weight, int topk, int block_size, Tensor? bias) -> ()
  variants: function

- func: fused_moe_quant_kernel(Tensor(a!) C, Tensor A, Tensor B, Tensor? A_scale, Tensor B_scale, int gs, Tensor? B_zp, Tensor topk_weights, Tensor topk_ids, Tensor sorted_token_ids, Tensor experts_ids, Tensor num_tokens_post_pad, bool mul_routed_weight, int topk, int block_size, Tensor? bias, Tensor? real_token_num) -> ()
  variants: function

- func: topk_softmax(Tensor(a!) topk_weights, Tensor(a!) topk_indices, Tensor(a!) token_expert_indices, Tensor(a!) gating_output) -> ()
  variants: function

- func: context_attention_forward(Tensor q, Tensor k, Tensor v, Tensor(a!) o, Tensor key_cache, Tensor value_cache, Tensor block_tables, Tensor subquery_start_loc, Tensor seq_lens_tensor, Tensor context_lens, int max_query_len, Tensor? alibi_slopes, int? sliding_window) -> ()
  variants: function

- func: weight_only_quant(Tensor(a!) output, Tensor input, Tensor qweight, Tensor? bias, Tensor scale, int group_size) -> ()
  variants: function

- func: rms_norm_static_int8_quant(Tensor(a!) output, Tensor input, Tensor weight, Tensor scale, float epsilon) -> ()
  variants: function

- func: fused_add_rms_norm_static_int8_quant(Tensor(a!) output, Tensor input, Tensor(a!) residual, Tensor weight, float epsilon, Tensor scale) -> ()
  variants: function

- func: gelu_tanh_static_int8_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_static_int8_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_new_static_int8_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: silu_static_int8_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_fast_static_int8_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_tanh_asym_quant(Tensor(a!) out, Tensor input, Tensor scale, Tensor qzero) -> ()
  variants: function

- func: gelu_asym_quant(Tensor(a!) out, Tensor input, Tensor scale, Tensor qzero) -> ()
  variants: function

- func: gelu_new_asym_quant(Tensor(a!) out, Tensor input, Tensor scale, Tensor qzero) -> ()
  variants: function

- func: silu_asym_quant(Tensor(a!) out, Tensor input, Tensor scale, Tensor qzero) -> ()
  variants: function

- func: gelu_fast_asym_quant(Tensor(a!) out, Tensor input, Tensor scale, Tensor qzero) -> ()
  variants: function

- func: static_scaled_int8_quant(Tensor(a!) output, Tensor input, Tensor scales, Tensor? azp) -> ()
  variants: function

- func: static_scaled_int8_dequant(Tensor(a!) output, Tensor input, Tensor scales) -> ()
  variants: function

- func: static_scaled_int8_asym_quant(Tensor(a!) output, Tensor input, Tensor scales, Tensor qzeros) -> ()
  variants: function

- func: static_scaled_int8_asym_dequant(Tensor(a!) output, Tensor input, Tensor scales, Tensor qzeros) -> ()
  variants: function

- func: silu_mul_static_int8_quant(Tensor(a!) result, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_mul_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: gelu_tanh_mul_quant(Tensor(a!) out, Tensor input, Tensor scale) -> ()
  variants: function

- func: layer_norm_static_int8_quant(Tensor(a!) output, Tensor input, Tensor scale, int[] normalized_shape, Tensor? weight, Tensor? bias, float epsilon) -> ()
  variants: function

- func: dot_bias_quant(Tensor(a!) out, Tensor lhs, Tensor rhs, Tensor scale, Tensor? bias) -> ()
  variants: function

- func: cutlass_scaled_mm(Tensor(a!) out, Tensor x, Tensor weight, Tensor x_scale, Tensor w_scale, Tensor? bias) -> ()
  variants: function

- func: memory_efficient_attention_alibi(Tensor(a!) output, Tensor query, Tensor key, Tensor value, Tensor alibi_slopes, float drop, float scale) -> ()
  variants: function

- func: dispatch_bgmv(Tensor(a!) y, Tensor x, Tensor w, Tensor indicies, int layer_idx, float scale) -> ()
  variants: function

- func: dispatch_bgmv_low_level(Tensor(a!) y, Tensor x, Tensor w, Tensor indicies, int layer_idx, float scale, int h_in, int h_out, int y_offset) -> ()
  variants: function

- func: advance_step_flashattn(int num_seqs, int num_queries, int block_size, Tensor(a!) input_tokens, Tensor(a!) sampled_token_ids, Tensor(a!) input_positions, Tensor(a!) seq_lens, Tensor(a!) slot_mapping, Tensor(a!) block_tables) -> ()
  variants: function

- func: advance_step_xformers(int num_seqs, int num_queries, int block_size, Tensor(a!) input_tokens, Tensor(a!) sampled_token_ids, Tensor(a!) input_positions, Tensor(a!) seq_lens, Tensor(a!) slot_mapping, Tensor(a!) block_tables) -> ()
  variants: function

- func: weak_ref_tensor(Tensor x) -> Tensor
  variants: function

- func: mul_and_silu(Tensor(a!) out, Tensor input) -> ()
  variants: function

- func: fatrelu_and_mul(Tensor(a!) out, Tensor input, float threshold) -> ()
  variants: function

- func: rms_norm_static_fp8_quant(Tensor(a!) result, Tensor input, Tensor weight, Tensor scale, float epsilon) -> ()
  variants: function

- func: fused_add_rms_norm_static_fp8_quant(Tensor(a!) result, Tensor input, Tensor(a!) residual, Tensor weight, Tensor scale, float epsilon) -> ()
  variants: function

- func: batched_rotary_embedding(Tensor positions, Tensor(a!) query, Tensor(a!) key, int head_size, Tensor cos_sin_cache, bool is_neox, int rot_dim, Tensor cos_sin_cache_offsets) -> ()
  variants: function

- func: concat_and_cache_mla(Tensor kv_c, Tensor k_pe, Tensor(a!) kv_cache, Tensor slot_mapping, str kv_cache_dtype, Tensor scale) -> ()
  variants: function

- func: fused_grouped_topk(Tensor(a!) topk_weights, Tensor(a!) topk_ids, Tensor gating_output, int topk, bool renormalize, int num_expert_group, int topk_group, Tensor e_score_correction_bias, str scoring_func) -> ()
  variants: function

- func: get_ep_indices(Tensor(a!) ep_count, Tensor(a!) ep_token_indices, Tensor(a!) ep_valid_token_indices, Tensor topk_ids, int expert_per_rank, int ep_size) -> ()
  variants: function

- func: silu_and_mul_pad(Tensor(a!) out, Tensor input, Tensor size) -> ()
  variants: function

- func: moe_sum(Tensor(a!) out, Tensor input, Tensor size, int dim, bool keepdim) -> ()
  variants: function

- func: linear_quant(Tensor(a!) out, Tensor lhs, Tensor rhs, Tensor? bias, Tensor lhs_scale, Tensor rhs_scale) -> ()
  variants: function

- func: dynamic_split(Tensor[] out, Tensor input, Tensor size, int[] split_sizes, int dim) -> ()
  variants: function

- func: dynamic_per_token_group_fp8_quant(Tensor(a!) out, Tensor(a!) scale, Tensor input, int group_size) -> ()
  variants: function

- func: silu_mul_per_token_group_quant(Tensor(a!) out, Tensor(a!) scale, Tensor input, int group_size) -> ()
  variants: function

- func: rms_norm_per_token_group_quant_fp8(Tensor(a!) out, Tensor(a!) scale, Tensor input, Tensor weight, float epsilon, int group_size) -> ()
  variants: function

- func: fused_add_rms_norm_per_token_group_quant_fp8(Tensor(a!) out, Tensor(a!) residual, Tensor(a!) scale, Tensor input, Tensor weight, float epsilon, int group_size) -> ()
  variants: function

- func: silu_mul_per_token_group_quant_with_size(Tensor(a!) out, Tensor(a!) scale, Tensor input, Tensor size, int group_size) -> ()
  variants: function

- func: rotary_embedding_with_kv_cache(Tensor(a!) q_out, Tensor(a!) kv_cache, Tensor q, Tensor kv, Tensor positions, Tensor cos_sin_cache, Tensor weight, Tensor slot_mapping, Tensor scale, float eps, int[] split_size, str kv_cache_dtype) -> ()
  variants: function

- func: fused_dispatch_decode(Tensor(a!)[] outputs, Tensor recv_packed, Tensor sp_split_size, int[] split_sizes) -> ()
  variants: function

- func: exts_moe_align_block_size(Tensor(a!) sorted_token_ids, Tensor(a!) experts_ids, Tensor(a!) num_tokens_post_pad, Tensor topk_ids, Tensor real_token_num, Tensor expert_map, int num_experts, int block_size) -> ()
  variants: function

- func: static_scaled_fp8_quant(Tensor(a!) result, Tensor input, Tensor scale) -> ()
  variants: function

- func: dynamic_scaled_fp8_quant(Tensor(a!) result, Tensor input, Tensor(a!) scale) -> ()
  variants: function

- func: dynamic_scaled_int8_quant(Tensor(a!) output, Tensor input, Tensor(a!) scales, Tensor azp) -> ()
  variants: function

- func: dynamic_per_token_scaled_fp8_quant(Tensor(a!) result, Tensor input, Tensor(a!) scale, Tensor? scale_ub) -> ()
  variants: function

- func: fused_qkv_proj(Tensor(a!) q, Tensor(a!) kv, Tensor x, Tensor weight, Tensor x_scale, Tensor weight_scale, int group_size) -> ()
  variants: function

- func: reshape_and_cache_flash(Tensor key, Tensor value, Tensor(a!) key_cache, Tensor(a!) value_cache, Tensor slot_mapping, str kv_cache_dtype, Tensor k_scale, Tensor v_scale) -> ()
  variants: function

- func: fused_qkv_gemm_quant(Tensor(a!) q, Tensor(a!) kv, Tensor x, Tensor weight, Tensor scale, Tensor zeros, int group_size) -> ()
  variants: function
  
- func: merge_attn_states(Tensor(a!) output, Tensor(a!) output_lse, Tensor prefix_output, Tensor prefix_lse, Tensor suffix_output, Tensor suffix_lse) -> ()
  variants: function
  
- func: gather_cache(Tensor src_cache, Tensor dst, Tensor block_table, Tensor cu_seq_lens, int batch_size, Tensor seq_starts) -> ()
  variants: function

- func: dynamic_per_token_group_fp8_quant_with_size(Tensor(a!) output, Tensor(a!) scale, Tensor input, Tensor real_size, int group_size) -> ()
  variants: function
